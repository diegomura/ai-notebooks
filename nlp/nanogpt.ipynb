{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT\n",
    "\n",
    "Character-based Transformer model trained over Shakespeare books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115390\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's read it\n",
    "text = open('tinyshakespeare.txt', 'r').read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for gpu support\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Get all unique characters that our dataset has\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[v] for v in i])\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115390]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now slip the data into a train and a validation dataset\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), target is 47\n",
      "when input is tensor([18, 47]), target is 56\n",
      "when input is tensor([18, 47, 56]), target is 57\n",
      "when input is tensor([18, 47, 56, 57]), target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print(f'when input is {context}, target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 51,  1, 44, 39, 47, 56, 12],\n",
      "        [46, 53, 53, 42,  1, 46, 43,  1],\n",
      "        [52, 48, 59, 56, 47, 53, 59, 57],\n",
      "        [47, 52, 45,  1, 47, 56, 53, 52]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[51,  1, 44, 39, 47, 56, 12,  0],\n",
      "        [53, 53, 42,  1, 46, 43,  1, 57],\n",
      "        [48, 59, 56, 47, 53, 59, 57,  1],\n",
      "        [52, 45,  1, 47, 56, 53, 52,  8]])\n",
      "---\n",
      "when input is [43], target is 51\n",
      "when input is [43, 51], target is 1\n",
      "when input is [43, 51, 1], target is 44\n",
      "when input is [43, 51, 1, 44], target is 39\n",
      "when input is [43, 51, 1, 44, 39], target is 47\n",
      "when input is [43, 51, 1, 44, 39, 47], target is 56\n",
      "when input is [43, 51, 1, 44, 39, 47, 56], target is 12\n",
      "when input is [43, 51, 1, 44, 39, 47, 56, 12], target is 0\n",
      "when input is [46], target is 53\n",
      "when input is [46, 53], target is 53\n",
      "when input is [46, 53, 53], target is 42\n",
      "when input is [46, 53, 53, 42], target is 1\n",
      "when input is [46, 53, 53, 42, 1], target is 46\n",
      "when input is [46, 53, 53, 42, 1, 46], target is 43\n",
      "when input is [46, 53, 53, 42, 1, 46, 43], target is 1\n",
      "when input is [46, 53, 53, 42, 1, 46, 43, 1], target is 57\n",
      "when input is [52], target is 48\n",
      "when input is [52, 48], target is 59\n",
      "when input is [52, 48, 59], target is 56\n",
      "when input is [52, 48, 59, 56], target is 47\n",
      "when input is [52, 48, 59, 56, 47], target is 53\n",
      "when input is [52, 48, 59, 56, 47, 53], target is 59\n",
      "when input is [52, 48, 59, 56, 47, 53, 59], target is 57\n",
      "when input is [52, 48, 59, 56, 47, 53, 59, 57], target is 1\n",
      "when input is [47], target is 52\n",
      "when input is [47, 52], target is 45\n",
      "when input is [47, 52, 45], target is 1\n",
      "when input is [47, 52, 45, 1], target is 47\n",
      "when input is [47, 52, 45, 1, 47], target is 56\n",
      "when input is [47, 52, 45, 1, 47, 56], target is 53\n",
      "when input is [47, 52, 45, 1, 47, 56, 53], target is 52\n",
      "when input is [47, 52, 45, 1, 47, 56, 53, 52], target is 8\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will be processed in parallel?\n",
    "block_size = 8 # whats the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "  # generate a small batch of data of inputs x and target y\n",
    "  data = train_data if split == 'train' else val_data\n",
    "\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  x, y = x.to(device), y.to(device)\n",
    "\n",
    "  return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('Inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('---')\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = xb[b][:t+1]\n",
    "    target = yb[b][t]\n",
    "    print(f'when input is {context.tolist()}, target is {target}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n",
    "\n",
    "Let's start feeding our batches into neural networks. The simplest possible neural network, which in the case of natural language one could be the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BingramLanguageModel(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # each token directly reads off the logits for the next token from a lookup table\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx and targets are both (B, T) tensor of integers\n",
    "    # in here we \"translate\" each character into it's embedding\n",
    "    logits = self.token_embedding_table(idx) # (Batch, Time, Channel)\n",
    "\n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      # cross_entropy expects (B, C, T)\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits.view(B*T, C), targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      # get the predictions\n",
    "      logits, loss = self(idx)\n",
    "      # focus only on the last time step. Hisotry not used because so far it's a bigram model\n",
    "      logits = logits[:, -1, :] # becomes (B, C)\n",
    "      # apply softmax to get the probabilities\n",
    "      probs = F.softmax(logits, dim=1) # (B, C)\n",
    "      # sample from distribution\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "      # append sampled index to the running sequence\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "# Create the model\n",
    "m = BingramLanguageModel(vocab_size)\n",
    "m = m.to(device)\n",
    "\n",
    "# Generate from model using 1 batch starting with char 0 (new line)\n",
    "out = m.generate(torch.tensor([[0]], device=device), max_new_tokens=100)\n",
    "\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/10000: train loss 2.4400, val loss 2.4826\n",
      "step 200/10000: train loss 2.4550, val loss 2.4828\n",
      "step 400/10000: train loss 2.4485, val loss 2.4927\n",
      "step 600/10000: train loss 2.4527, val loss 2.4841\n",
      "step 800/10000: train loss 2.4536, val loss 2.4784\n",
      "step 1000/10000: train loss 2.4503, val loss 2.4848\n",
      "step 1200/10000: train loss 2.4507, val loss 2.4898\n",
      "step 1400/10000: train loss 2.4535, val loss 2.4843\n",
      "step 1600/10000: train loss 2.4592, val loss 2.4899\n",
      "step 1800/10000: train loss 2.4554, val loss 2.4811\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_iters = 10000\n",
    "eval_iters = 200\n",
    "eval_interval = 2000\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "  out = {}\n",
    "\n",
    "  # put our model into evaluation phase\n",
    "  m.eval()\n",
    "\n",
    "  for split in ['train', 'val']:\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "      X, Y = get_batch(split)\n",
    "      logits, loss = m(X, Y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "\n",
    "  # resetting our model to training phase\n",
    "  m.train()\n",
    "\n",
    "  return out\n",
    "\n",
    "\n",
    "for step in range(eval_interval):\n",
    "  # every once in awhile evaluate the loss on train and val set using batches mean\n",
    "  if step % eval_iters == 0:\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step {step}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "  # sample a batch of data\n",
    "  xb, yb = get_batch('train')\n",
    "\n",
    "  # evaluate the loss\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wherorusoorerf by byothotht y go ie moserexis.\n",
      "\n",
      "Wedatthierth?\n",
      "\n",
      "NO: cake teerersomp'dve o theshe charher lay ht Is athanoutod, s ond my ah wak tosooupo,\n",
      "CAUKELoul-be imanson pis!'boimours'tu ppous CHemy d ameakeairce\n",
      "Clt r s t men MESTHAnoug, me\n",
      "Whickeat wisouthapr ar l.\n",
      "Ad laspay antar been then'coeoarer, AESond, froukint usord uy m, acheaid, y Cuse\n",
      "I s twoudin keay athoully'sel lif tharery avorshe very hortho antelee f t ftof chefons ise, an lakefouitlat tadack ES: yo my Sirvedenor,\n",
      "Wime i\n",
      "DO:\n",
      "\n",
      "IOUS S:\n",
      "\n",
      "I iowhe thiveme thee n drro ofofecherent cu sa ceancedenof tithay me ng mpo thitiothavy ou\n"
     ]
    }
   ],
   "source": [
    "# fetch from model again\n",
    "\n",
    "# Generate from model using 1 batch starting with char 0 (new line)\n",
    "out = m.generate(torch.tensor([[0]], device=device), max_new_tokens=600)\n",
    "\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Trick on Self-Attention\n",
    "\n",
    "Before jumping into transformers, it's handy to understand a trick that's crucial on the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the following toy example:\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "c=\n",
      "tensor([[5.0000, 7.0000],\n",
      "        [3.5000, 3.5000],\n",
      "        [4.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "# efficient way to compute average using matrix multiplication\n",
    "torch.manual_seed(1337)\n",
    "a=torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the latter\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
    "xbox = wei @ x # (T, T) @ (B, T, C) --> (B, T, C)\n",
    "\n",
    "# which is the same\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei2 = torch.zeros(T, T)\n",
    "wei2 = wei2.masked_fill(tril == 0, float('-inf'))\n",
    "wei2 = F.softmax(wei2, dim=0)\n",
    "xbox2 = wei2 @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5000, 3.5000, 4.5000])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor([[1,2,3], [4,5,6]], dtype=torch.float), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Our Model\n",
    "\n",
    "We can see that our model improved, but obviously it's not good. In the Bigram model the tokens are not talking to each other. One char is only being redicted based on the previous one. We need to start making talk and use the whole context!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
